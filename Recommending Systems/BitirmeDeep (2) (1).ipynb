{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezg1W3fHYcf1"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from keras.layers import Dropout, BatchNormalization, Activation\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# Veriyi yükleyelim.\n",
        "df = pd.read_csv('edited_rating.csv')\n",
        "\n",
        "# N kullanıcıların sayısı\n",
        "N = df.userId.max() + 1\n",
        "# Filmlerin sayısı\n",
        "M = df.movie_idx.max() + 1\n",
        "\n",
        "# Veriyi test ve eğitim verisi olarak ayıralım.\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "K = 10 # Gizli boyutluluk(Latent Dimensionality)\n",
        "mu = df_train.rating.mean()\n",
        "epochs = 15\n",
        "#reg = 0.0001 # regularizasyon uygulayarak maliyet fonksiyonuna ekstra bir terim ekleyerek maliyeti artırmak için kullanıyoruz.\n",
        "#Regularization sayesinde overfittingi yani ezberleme problemini engelleyebiliriz.\n",
        "\n",
        "\n",
        "# keras modelimiz.\n",
        "u = Input(shape=(1,))\n",
        "m = Input(shape=(1,))\n",
        "#Embedding fonksiyonu, verinin yüksek boyutlu uzayını daha düşük boyutlu bir uzaya dönüştürerek boyut indirgeme sağlar.\n",
        "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
        "m_embedding = Embedding(N, K)(m) # (N, 1, K)\n",
        "#Boyutlarını indirgedikten sonra inputlarımızı düzleştirelim.Böylece modelimizi eğitebiliriz.\n",
        "u_embedding = Flatten()(u_embedding) # (N, K)\n",
        "m_embedding = Flatten()(m_embedding) # (N, K)\n",
        "#Concantinate, birleştirme demek. Aslında her bir yoldan gelen, özellik haritalarını birleştiriyor. Buna concantinate deniyor.\n",
        "x = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
        "#Artık x input olarak hazır.\n",
        "\n",
        "# Modelimizi tasarlayalım.400'lük bir Dense katmanı oluşturduk.Girdi olarak x'i aldı.X'i girdileri düzenleyerek elde etmiştik.\n",
        "x = Dense(400)(x)\n",
        "#Burada BatchNormalization'da yapabiliriz.BatchNormalization sinir ağlarının eğitimini hızlandırır,\n",
        "#daha iyi bir genelleme sağlar ve modelin daha kararlı ve güvenilir olmasını sağlar\n",
        "# x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "#Dropout'ta tıpkı Regularization gibi overfittingi engelleme yöntemlerinden birisi.\n",
        "#Modelimize 100'lük bir dense katmanı daha ekleyerek modeli daha karmaşık hale getirebiliriz.\n",
        "#Bu sayede model daha iyi öğrenebilir.Bunu deneyerek test etmeliyiz.\n",
        "# x = Dense(100)(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Activation('relu')(x)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "#Modelimize girdileri ve çıktıyı veriyoruz.\n",
        "model = Model(inputs=[u, m], outputs=x)\n",
        "#Modeli derliyoruz.Optimizer olarak adam'ı kullandık.\n",
        "#Adam yerine sgd optimizeri'da deneyerek hangisi daha iyi sonuç veriyor görebiliriz.\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  # optimizer='adam',\n",
        "  # optimizer=Adam(lr=0.01),\n",
        "  optimizer='adam',\n",
        "  metrics=['mse'],\n",
        ")\n",
        "\n",
        "#Modeli eğitmeye başlıyoruz.128'lik batchlar halinde veriyi alıp işliyor.\n",
        "#Modeli 15 epochta eğiteceğiz.\n",
        "r = model.fit(\n",
        "  x=[df_train.userId.values, df_train.movie_idx.values],\n",
        "  y=df_train.rating.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=128,\n",
        "  validation_data=(\n",
        "    [df_test.userId.values, df_test.movie_idx.values],\n",
        "    df_test.rating.values - mu\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "# modelin test ve eğitim hata sonuçlarını tabloda görelim.\n",
        "plt.plot(r.history['loss'], label=\"train loss\")\n",
        "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}